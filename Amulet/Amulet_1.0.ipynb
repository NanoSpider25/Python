{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1R7efQKlSBAyAI1hKZTuFgyQRiTbMGdJC","timestamp":1741509134470}],"authorship_tag":"ABX9TyNjuDIxwdI74JeHVJsl9o0z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torch transformers"],"metadata":{"id":"nNUOQjAXeaFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQRiFGey4SCn"},"outputs":[],"source":["import torch\n","import time\n","import os\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from torch.utils.data import Dataset, DataLoader\n","from google.colab import drive\n","\n","# Define a simple dataset for training\n","class SimpleDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_length=128):\n","        tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS\n","        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length)\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])\n","\n","    def __getitem__(self, idx):\n","        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n","\n","drive.mount('/content/drive')\n","\n","# Load pretrained GPT-2 model and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/My Drive/GPT-2/Amulet_1.0/outputTest_checkpoint')\n","model = GPT2LMHeadModel.from_pretrained('/content/drive/My Drive/GPT-2/Amulet_1.0/outputTest_checkpoint')\n","\n","# Check if GPU is available and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)  # Move model to GPU if available\n","\n","\n","qa_data = []\n","\n","# Read the file\n","with open(\"/content/drive/My Drive/GPT-2/Amulet_1.0/outputTest.txt\", \"r\") as file:\n","    lines = file.read().splitlines()\n","\n","    # Loop through the lines to store data\n","    for line in lines:\n","        qa_data.append((line))\n","\n","dataset = SimpleDataset(qa_data, tokenizer)\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# Set up the optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","\n","# Set model to training mode\n","model.train()\n","\n","# Training loop (mock training)\n","i = 0\n","c = time.time_ns()/(60*1e9)\n","for batch in dataloader:\n","    # Move input tensors to GPU if available\n","    input_ids = batch[\"input_ids\"].to(device)\n","    attention_mask = batch[\"attention_mask\"].to(device)\n","\n","    # Zero the gradients\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n","    loss = outputs.loss\n","\n","    # Backward pass\n","    loss.backward()\n","\n","    # Update weights\n","    optimizer.step()\n","\n","    if(i % 1000 == 0):\n","        i = 0\n","        print(f\"Loss: {loss.item()} Time Elapsed: {(time.time_ns()/(60*1e9) - c):.9f}\")  # Print loss for monitoring\n","        model.save_pretrained('/content/drive/My Drive/GPT-2/Amulet_1.0/outputTest_checkpoint')\n","        tokenizer.save_pretrained('/content/drive/My Drive/GPT-2/Amulet_1.0/outputTest_checkpoint')\n","        c = time.time_ns()/(60*1e9)\n","    i += 1\n","\n","# Save the final model and tokenizer\n","model.save_pretrained('/content/drive/My Drive/GPT-2/Amulet_1.0')\n","tokenizer.save_pretrained('/content/drive/My Drive/GPT-2/Amulet_1.0')\n"]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from google.colab import drive\n","\n","# Load the trained model and tokenizer\n","#model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","#model.eval()\n","\n","# Function to ask a question and get an answer\n","def ask_question(question):\n","    input_text = f\"Question: {question} Answer:\"\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n","\n","    with torch.no_grad():\n","        output = model.generate(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            max_length=50,\n","            num_return_sequences=1,\n","            no_repeat_ngram_size=2,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.95,\n","            temperature=0.7\n","        )\n","\n","    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return answer.split(\"Answer:\")[-1].strip()  # Return the generated answer\n","\n","# Example questions\n","#print(ask_question(\"Where am I?\"))\n","\n","drive.mount('/content/drive')\n","\n","\n","model = GPT2LMHeadModel.from_pretrained(\"/content/drive/My Drive/GPT-2/Amulet_1.0/outputTest_checkpoint\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"/content/drive/My Drive/GPT-2/Amulet_1.0/outputTest_checkpoint\")\n","model.eval()\n","\n","\n","print(ask_question(\"Who has the Bing Chilling?\"))\n","\n"],"metadata":{"id":"0rC1rpGz5G9B"},"execution_count":null,"outputs":[]}]}